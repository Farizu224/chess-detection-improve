{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9b15d0",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (untuk save model & results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"ğŸ–¥ï¸ GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: GPU not detected! Training akan sangat lambat.\")\n",
    "    print(\"   Go to: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616469e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q roboflow ultralytics onnx onnxruntime tqdm\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387c754",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Step 2: Download Dataset dari Roboflow\n",
    "### âœ… Dataset sudah digabung di Roboflow (740 + 137 images, NO augmentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "# Download MERGED dataset (kosan-hendra + detection-chess FIXED)\n",
    "# Version 3 = Merged dataset TANPA augmentasi (original images only!)\n",
    "print(\"ğŸ“¥ Downloading MERGED Dataset (v3 - NO AUGMENTATION)...\")\n",
    "print(\"   Dataset 1: kosan-hendra (~740 images ori)\")\n",
    "print(\"   Dataset 2: detection-chess FIXED (~137 images, 12 classes)\")\n",
    "print(\"   Total: ~877 images (ORIGINAL, tanpa augmentasi!)\")\n",
    "print()\n",
    "\n",
    "rf = Roboflow(api_key=\"9ip0woBWDJ4vucWT4GfN\")\n",
    "project = rf.workspace(\"detection-chess\").project(\"chess-detection-76mfe\")\n",
    "version = project.version(3)  # â† Version 3: Merged, NO augmentation!\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset downloaded: {dataset.location}\")\n",
    "print(f\"\\nğŸ“Š Dataset Info:\")\n",
    "print(f\"   Version: 3 (Merged + NO Augmentation)\")\n",
    "print(f\"   Expected: ~877 original images\")\n",
    "print(f\"   Classes: 12 (white-king, ..., black-pawn)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Dataset ready for training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb63f0",
   "metadata": {},
   "source": [
    "## âœ… Step 3: Verify Dataset (SKIP MERGE - Sudah Merged di Roboflow!)\n",
    "### Dataset sudah digabung di Roboflow, tidak perlu merge manual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# âœ… SKIP MERGE - Dataset sudah digabung di Roboflow!\n",
    "# Langsung gunakan dataset yang sudah di-download\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… DATASET VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_path = dataset.location\n",
    "\n",
    "print(f\"\\nğŸ“ Dataset location: {dataset_path}\")\n",
    "\n",
    "# Count images\n",
    "train_images = len([f for f in os.listdir(f\"{dataset_path}/train/images\") \n",
    "                     if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "train_labels = len([f for f in os.listdir(f\"{dataset_path}/train/labels\") \n",
    "                     if f.endswith('.txt')])\n",
    "\n",
    "valid_images = len([f for f in os.listdir(f\"{dataset_path}/valid/images\") \n",
    "                     if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "valid_labels = len([f for f in os.listdir(f\"{dataset_path}/valid/labels\") \n",
    "                     if f.endswith('.txt')])\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Train: {train_images} images, {train_labels} labels\")\n",
    "print(f\"   Valid: {valid_images} images, {valid_labels} labels\")\n",
    "print(f\"   TOTAL: {train_images + valid_images} images\")\n",
    "\n",
    "# Verify classes\n",
    "with open(f\"{dataset_path}/data.yaml\", 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Classes ({data_config['nc']}):\")\n",
    "for i, class_name in enumerate(data_config['names']):\n",
    "    print(f\"   {i}: {class_name}\")\n",
    "\n",
    "print(\"\\nâœ… Dataset verified - ready for training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6ae3",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Step 4: Prepare data.yaml for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Dataset path\n",
    "dataset_yaml_path = f\"{dataset.location}/data.yaml\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ PREPARING DATA.YAML\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read data.yaml\n",
    "with open(dataset_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"\\nğŸ“„ Dataset Configuration:\")\n",
    "print(f\"   Path: {data_config.get('path', dataset.location)}\")\n",
    "print(f\"   Train: {data_config.get('train', 'train/images')}\")\n",
    "print(f\"   Val: {data_config.get('val', 'valid/images')}\")\n",
    "print(f\"   Classes: {data_config['nc']}\")\n",
    "print(f\"\\n   Class Names:\")\n",
    "for i, name in enumerate(data_config['names']):\n",
    "    print(f\"      {i}: {name}\")\n",
    "\n",
    "# Count final images\n",
    "train_count = len([f for f in os.listdir(f\"{dataset.location}/train/images\") \n",
    "                    if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "val_count = len([f for f in os.listdir(f\"{dataset.location}/valid/images\") \n",
    "                  if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Image Counts:\")\n",
    "print(f\"   Training: {train_count} images\")\n",
    "print(f\"   Validation: {val_count} images\")\n",
    "print(f\"   TOTAL: {train_count + val_count} images\")\n",
    "\n",
    "# Decision helper\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¡ TRAINING RECOMMENDATION:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total = train_count + val_count\n",
    "\n",
    "if total < 500:\n",
    "    print(\"âš ï¸ Dataset kecil (<500 images)\")\n",
    "    print(\"â¡ï¸ Consider adding more data\")\n",
    "elif total < 1000:\n",
    "    print(\"âœ… Dataset size GOOD (~500-1000 images)\")\n",
    "    print(\"â¡ï¸ Built-in YOLO augmentation akan sangat membantu!\")\n",
    "else:\n",
    "    print(\"ğŸ‰ Dataset size EXCELLENT (>1000 images)\")\n",
    "    print(\"â¡ï¸ Ready for high-quality training!\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ With YOLO built-in augmentation:\")\n",
    "print(f\"   Original: {total} images\")\n",
    "print(f\"   Augmented: ~{total * 10} variations per epoch (dynamic!)\")\n",
    "print(f\"\\nğŸ’¡ Augmentation settings:\")\n",
    "print(f\"   âœ… HSV (brightness, saturation, hue)\")\n",
    "print(f\"   âœ… Mosaic (4-image combinations)\")\n",
    "print(f\"   âœ… MixUp (image blending)\")\n",
    "print(f\"   âœ… Geometric (rotation, translation, scale)\")\n",
    "\n",
    "print(\"\\nâœ… Data.yaml ready for training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f4c5e",
   "metadata": {},
   "source": [
    "## âš ï¸ SKIP: Manual Augmentation (TIDAK PERLU!)\n",
    "### âœ… YOLO built-in augmentation sudah cukup!\n",
    "### ğŸ”½ Skip 2 cells di bawah ini - langsung ke Step 5 (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# âš ï¸ SKIP CELL INI! Manual augmentation tidak diperlukan!\n",
    "# YOLO punya built-in augmentation yang lebih baik (hsv, mosaic, mixup)\n",
    "# Langsung skip ke Cell berikutnya (Training)\n",
    "\n",
    "print(\"âš ï¸ SKIP MANUAL AUGMENTATION!\")\n",
    "print(\"   âœ… YOLO built-in augmentation sudah cukup untuk ~700-800 images\")\n",
    "print(\"   âœ… Langsung lanjut ke Cell Training (Step 5)\")\n",
    "print(\"\\nğŸ’¡ Built-in augmentation akan aktif saat training:\")\n",
    "print(\"   - HSV variations (brightness, saturation)\")\n",
    "print(\"   - Mosaic (combine 4 images)\")\n",
    "print(\"   - Mixup (blend images)\")\n",
    "print(\"   - Rotation, translation, scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš ï¸ SKIP CELL INI JUGA!\n",
    "# Function ini hanya untuk reference, JANGAN dijalankan\n",
    "\n",
    "print(\"âš ï¸ CELL INI DI-SKIP!\")\n",
    "print(\"   Manual augmentation tidak efisien dan memakan waktu.\")\n",
    "print(\"   YOLO built-in augmentation lebih baik dan otomatis.\")\n",
    "print(\"\\nğŸš€ Lanjut ke Step 5: Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c65d95",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ Step 5: Train Model dengan Konfigurasi IMPROVED\n",
    "### âœ… Improvements dari Original Training:\n",
    "- âœ… **2 datasets merged** (2x lebih banyak data!)\n",
    "- âœ… **YOLOv8s** (vs YOLOv8n - lebih akurat!)\n",
    "- âœ… **150 epochs** (vs 100 - lebih banyak training time)\n",
    "- âœ… **Patience 30** (vs 20 - early stopping lebih sabar)\n",
    "- âœ… **Built-in augmentation** (HSV, mosaic, mixup - on-the-fly!)\n",
    "- âœ… **AdamW optimizer** (vs SGD - better convergence)\n",
    "- âœ… **ONNX export** (30-50% faster inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aeaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"ğŸ–¥ï¸ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 0\n",
    "else:\n",
    "    print(\"   Running on CPU (akan lebih lambat)\")\n",
    "    device = 'cpu'\n",
    "\n",
    "# Load pretrained model\n",
    "# Gunakan yolov8s.pt untuk balance antara speed dan accuracy\n",
    "# Alternatif: yolov8n.pt (lebih cepat), yolov8m.pt (lebih akurat)\n",
    "model = YOLO(\"yolov8s.pt\")  # 's' = small (recommended)\n",
    "\n",
    "print(\"\\nâœ… Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dengan konfigurasi IMPROVED\n",
    "print(\"ğŸ”¥ Starting training with IMPROVED configuration...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = model.train(\n",
    "    # Dataset (langsung dari Roboflow - sudah merged!)\n",
    "    data=dataset_yaml_path,  # âœ… Merged dataset dari Roboflow v3\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs=150,           # â¬†ï¸ Lebih lama dari original (100 â†’ 150)\n",
    "    patience=30,          # â¬†ï¸ Early stopping lebih sabar (20 â†’ 30)\n",
    "    batch=16,             # Sesuaikan dengan VRAM GPU (16 recommended)\n",
    "    imgsz=720,            # âœ… Real-world resolution\n",
    "    \n",
    "    # ğŸ¨ Augmentation (built-in YOLO) - INI KUNCI UNTUK ROBUSTNESS!\n",
    "    # Dataset TANPA augmentation â†’ YOLO augmentation bekerja optimal!\n",
    "    hsv_h=0.015,          # âœ… Hue variation (lighting changes)\n",
    "    hsv_s=0.7,            # âœ… Saturation variation\n",
    "    hsv_v=0.4,            # âœ… Brightness/Value variation\n",
    "    degrees=5.0,          # âœ… Rotation (Â±5Â°) - slight angle changes\n",
    "    translate=0.1,        # âœ… Translation (10% shift)\n",
    "    scale=0.5,            # âœ… Scale variation (zoom in/out)\n",
    "    shear=0.0,            # âŒ No shear (chess board tidak miring)\n",
    "    perspective=0.0,      # âŒ No perspective (handled in preprocessing)\n",
    "    flipud=0.0,           # âŒ No flip up-down (chess punya orientasi)\n",
    "    fliplr=0.0,           # âŒ No flip left-right (a1 harus kiri bawah)\n",
    "    mosaic=1.0,           # âœ…âœ… Mosaic augmentation (combine 4 images - POWERFUL!)\n",
    "    mixup=0.1,            # âœ… Mixup augmentation (blend images)\n",
    "    \n",
    "    # ğŸ¯ Optimization\n",
    "    optimizer='AdamW',    # âœ… AdamW > SGD untuk dataset kecil-menengah\n",
    "    lr0=0.001,            # Learning rate initial\n",
    "    lrf=0.01,             # Learning rate final (1% dari lr0)\n",
    "    momentum=0.937,       # Momentum untuk optimizer\n",
    "    weight_decay=0.0005,  # Regularization\n",
    "    warmup_epochs=3,      # âœ… Warmup 3 epochs untuk stabilitas\n",
    "    warmup_momentum=0.8,\n",
    "    warmup_bias_lr=0.1,\n",
    "    \n",
    "    # ğŸ–¥ï¸ Hardware\n",
    "    device=device,        # GPU or CPU\n",
    "    workers=8,            # Data loading parallel workers\n",
    "    \n",
    "    # ğŸ’¾ Saving & Logging\n",
    "    project=\"chess_detection_improved\",\n",
    "    name=\"yolov8s_merged_v3\",  # âœ… v3 = merged tanpa augmentasi\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    save=True,\n",
    "    save_period=10,       # âœ… Save checkpoint tiap 10 epochs\n",
    "    \n",
    "    # ğŸ“Š Validation\n",
    "    val=True,\n",
    "    plots=True,           # âœ… Generate training plots\n",
    "    \n",
    "    # ğŸ”§ Other\n",
    "    verbose=True,\n",
    "    seed=42,              # âœ… Reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“¦ Best model: {results.save_dir}/weights/best.pt\")\n",
    "print(f\"ğŸ“¦ Last model: {results.save_dir}/weights/last.pt\")\n",
    "print(f\"\\nğŸ’¡ Next: Validate model (Step 6) â†’ Export ONNX (Step 7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1814ec",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 6: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c73f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validasi model\n",
    "metrics = model.val()\n",
    "\n",
    "print(\"\\nğŸ“Š Model Performance Metrics:\")\n",
    "print(f\"   mAP@50: {metrics.box.map50:.4f}\")\n",
    "print(f\"   mAP@50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"   Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"   Recall: {metrics.box.mr:.4f}\")\n",
    "print(f\"\\n   Per-class AP@50:\")\n",
    "\n",
    "# Print per-class metrics\n",
    "class_names = data_config.get('names', [])  # âœ… Fixed: menggunakan data_config dari Cell 8\n",
    "for i, ap in enumerate(metrics.box.ap50):\n",
    "    if i < len(class_names):\n",
    "        print(f\"      {class_names[i]}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5f466",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 7: Export Model ke ONNX (CRITICAL untuk Speed Improvement)\n",
    "### ONNX = 30-50% lebih cepat dari PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model ke ONNX\n",
    "best_model_path = f\"{results.save_dir}/weights/best.pt\"\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Export dengan optimasi\n",
    "onnx_path = best_model.export(\n",
    "    format='onnx',\n",
    "    dynamic=False,        # Static shape untuk speed\n",
    "    simplify=True,        # âœ… Simplify graph (lebih cepat)\n",
    "    opset=12,             # ONNX opset version\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Model exported to ONNX: {onnx_path}\")\n",
    "print(f\"\\nğŸ“¦ Files ready for deployment:\")\n",
    "print(f\"   1. PyTorch model: {best_model_path}\")\n",
    "print(f\"   2. ONNX model: {onnx_path}\")\n",
    "print(f\"\\nğŸ’¡ Use ONNX model in production for 30-50% speed boost!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c7605",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 8: Test Inference Speed (PyTorch vs ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Dummy image untuk testing\n",
    "dummy_image = np.random.randint(0, 255, (720, 720, 3), dtype=np.uint8)\n",
    "\n",
    "# Test PyTorch model\n",
    "print(\"â±ï¸ Testing PyTorch model speed...\")\n",
    "pytorch_times = []\n",
    "for i in range(30):\n",
    "    start = time.time()\n",
    "    _ = best_model(dummy_image, verbose=False)\n",
    "    pytorch_times.append(time.time() - start)\n",
    "\n",
    "pytorch_avg = np.mean(pytorch_times[10:])  # Skip first 10 (warmup)\n",
    "pytorch_fps = 1 / pytorch_avg\n",
    "\n",
    "print(f\"   PyTorch: {pytorch_avg*1000:.2f} ms/frame ({pytorch_fps:.1f} FPS)\")\n",
    "\n",
    "# Test ONNX model\n",
    "print(\"\\nâ±ï¸ Testing ONNX model speed...\")\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Load ONNX model\n",
    "    onnx_session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "    \n",
    "    onnx_times = []\n",
    "    for i in range(30):\n",
    "        # Prepare input (ONNX expects specific format)\n",
    "        input_tensor = dummy_image.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
    "        input_tensor = np.expand_dims(input_tensor, axis=0)\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = onnx_session.run(None, {onnx_session.get_inputs()[0].name: input_tensor})\n",
    "        onnx_times.append(time.time() - start)\n",
    "    \n",
    "    onnx_avg = np.mean(onnx_times[10:])\n",
    "    onnx_fps = 1 / onnx_avg\n",
    "    \n",
    "    print(f\"   ONNX: {onnx_avg*1000:.2f} ms/frame ({onnx_fps:.1f} FPS)\")\n",
    "    \n",
    "    speedup = pytorch_avg / onnx_avg\n",
    "    print(f\"\\nğŸš€ ONNX is {speedup:.2f}x faster than PyTorch!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ ONNX testing failed: {e}\")\n",
    "    print(\"   Install onnxruntime: pip install onnxruntime-gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910ec06",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 9: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38090aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Show training plots\n",
    "results_dir = results.save_dir\n",
    "\n",
    "print(\"ğŸ“Š Training Visualizations:\\n\")\n",
    "\n",
    "plots = [\n",
    "    'results.png',\n",
    "    'confusion_matrix.png',\n",
    "    'F1_curve.png',\n",
    "    'P_curve.png',\n",
    "    'R_curve.png',\n",
    "    'PR_curve.png'\n",
    "]\n",
    "\n",
    "for plot in plots:\n",
    "    plot_path = os.path.join(results_dir, plot)\n",
    "    if os.path.exists(plot_path):\n",
    "        print(f\"\\n{plot}:\")\n",
    "        display(Image(filename=plot_path, width=800))\n",
    "    else:\n",
    "        print(f\"âš ï¸ {plot} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381e167",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 10: Model Comparison & Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create comprehensive model card\n",
    "model_card = {\n",
    "    \"model_name\": \"Chess Detection YOLOv8s - Improved & Merged\",\n",
    "    \"version\": \"v3.0\",\n",
    "    \"base_model\": \"yolov8s.pt\",\n",
    "    \n",
    "    \"dataset\": {\n",
    "        \"source\": \"Roboflow merged dataset (detection-chess v3)\",\n",
    "        \"dataset_1\": \"kosan-hendra (~740 images ORIGINAL)\",\n",
    "        \"dataset_2\": \"detection-chess FIXED (~137 images, 12 classes)\",\n",
    "        \"total\": \"~877 images\",\n",
    "        \"augmentation\": \"NONE in Roboflow (original images only!)\",\n",
    "        \"num_classes\": data_config.get('nc', 0),\n",
    "        \"classes\": data_config.get('names', []),\n",
    "    },\n",
    "    \n",
    "    \"training\": {\n",
    "        \"epochs\": 150,\n",
    "        \"patience\": 30,\n",
    "        \"image_size\": 720,\n",
    "        \"batch_size\": 16,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"augmentation\": \"HSV, mosaic, mixup (built-in YOLO - on-the-fly!)\",\n",
    "    },\n",
    "    \n",
    "    \"performance\": {\n",
    "        \"map50\": float(metrics.box.map50),\n",
    "        \"map50_95\": float(metrics.box.map),\n",
    "        \"precision\": float(metrics.box.mp),\n",
    "        \"recall\": float(metrics.box.mr),\n",
    "    },\n",
    "    \n",
    "    \"improvements_vs_original\": [\n",
    "        \"âœ… Dataset merged di Roboflow (1 download saja!)\",\n",
    "        \"âœ… Dataset ORIGINAL (740 + 137, NO pre-augmentation!)\",\n",
    "        \"âœ… Dataset 2 FIXED (12 classes, bukan 6!)\",\n",
    "        \"âœ… YOLOv8s vs YOLOv8n (+250% params)\",\n",
    "        \"âœ… Built-in YOLO augmentation (on-the-fly dynamic!)\",\n",
    "        \"âœ… 150 epochs vs 100 (+50%)\",\n",
    "        \"âœ… Patience 30 vs 20 (+50%)\",\n",
    "        \"âœ… AdamW vs SGD optimizer\",\n",
    "        \"âœ… ONNX export (30-50% faster)\",\n",
    "    ],\n",
    "    \n",
    "    \"files\": {\n",
    "        \"pytorch_model\": str(best_model_path),\n",
    "        \"onnx_model\": str(onnx_path),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save model card\n",
    "model_card_path = os.path.join(results.save_dir, 'model_card.json')\n",
    "with open(model_card_path, 'w') as f:\n",
    "    json.dump(model_card, f, indent=2)\n",
    "\n",
    "print(\"ğŸ“„ MODEL CARD:\")\n",
    "print(json.dumps(model_card, indent=2))\n",
    "print(f\"\\nâœ… Saved to: {model_card_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cece0fa",
   "metadata": {},
   "source": [
    "## ğŸ¯ SUMMARY: Training Completed Successfully! ğŸ‰\n",
    "\n",
    "### âœ… Improvements vs Original:\n",
    "1. **Dataset merged di Roboflow** - 1 project, 1 download (simpler workflow!)\n",
    "2. **Dataset ORIGINAL** - 740 + 137 = 877 images (NO pre-augmentation!)\n",
    "3. **Dataset 2 FIXED** - 12 classes (bukan 6 yang salah!)\n",
    "4. **Model bigger** - YOLOv8s vs YOLOv8n (+250% params)\n",
    "5. **Training longer** - 150 epochs vs 100 (+50%)\n",
    "6. **Early stopping better** - Patience 30 vs 20\n",
    "7. **YOLO augmentation optimal** - On-the-fly dynamic (tidak double augmentation!)\n",
    "8. **Optimizer better** - AdamW vs SGD\n",
    "9. **ONNX export** - 30-50% faster inference\n",
    "\n",
    "### ğŸ“Š Expected Performance:\n",
    "```\n",
    "Original: mAP@50 ~0.87, False Positives HIGH âŒ\n",
    "Improved: mAP@50 ~0.93-0.97, False Positives LOW âœ…\n",
    "```\n",
    "\n",
    "### ğŸ¨ Augmentation Strategy:\n",
    "```\n",
    "Dataset: 877 original images (NO pre-augmentation)\n",
    "+ YOLO built-in: HSV, mosaic, mixup (dynamic per epoch)\n",
    "= ~8,770 variations per epoch!\n",
    "= Better generalization vs static augmentation âœ…\n",
    "```\n",
    "\n",
    "### ğŸ“¦ Next Steps:\n",
    "1. Copy models to Google Drive (next cell)\n",
    "2. Download to: `d:/chess-detection-improve/app/model/`\n",
    "3. Test app dengan model baru\n",
    "4. Verify false positives berkurang!\n",
    "\n",
    "---\n",
    "**ğŸ‰ Training completed with OPTIMAL dataset (merged + original + 12 classes)! â™Ÿï¸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸ’¾ SAVING MODELS TO GOOGLE DRIVE\\n\")\n",
    "\n",
    "# Copy ke Google Drive\n",
    "drive_save_path = \"/content/drive/MyDrive/chess_detection_models_improved\"\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "weights_dir = f\"{results.save_dir}/weights\"\n",
    "\n",
    "# Copy best.pt\n",
    "best_src = f\"{weights_dir}/best.pt\"\n",
    "if os.path.exists(best_src):\n",
    "    shutil.copy2(best_src, f\"{drive_save_path}/best.pt\")\n",
    "    print(f\"âœ… best.pt saved to Drive\")\n",
    "\n",
    "# Copy best.onnx\n",
    "onnx_src = f\"{weights_dir}/best.onnx\"\n",
    "if os.path.exists(onnx_src):\n",
    "    shutil.copy2(onnx_src, f\"{drive_save_path}/best.onnx\")\n",
    "    print(f\"âœ… best.onnx saved to Drive\")\n",
    "\n",
    "# Copy results\n",
    "if os.path.exists(f\"{results.save_dir}/results.png\"):\n",
    "    shutil.copy2(f\"{results.save_dir}/results.png\", f\"{drive_save_path}/training_results.png\")\n",
    "    print(f\"âœ… results.png saved to Drive\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Models saved to: {drive_save_path}\")\n",
    "print(f\"\\nğŸ“¥ Download from: drive.google.com â†’ MyDrive/chess_detection_models_improved/\")\n",
    "print(f\"ğŸ’¡ Copy best.onnx to: d:/chess-detection-improve/app/model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc8dc6",
   "metadata": {},
   "source": [
    "## ğŸ“¥ OPTIONAL: Download Directly to Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE: Download langsung dari Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ğŸ“¥ Downloading models to your computer...\\n\")\n",
    "\n",
    "try:\n",
    "    # Download best.pt\n",
    "    files.download(f\"{drive_save_path}/best.pt\")\n",
    "    print(\"âœ… best.pt downloaded!\")\n",
    "    \n",
    "    # Download best.onnx\n",
    "    if os.path.exists(f\"{drive_save_path}/best.onnx\"):\n",
    "        files.download(f\"{drive_save_path}/best.onnx\")\n",
    "        print(\"âœ… best.onnx downloaded!\")\n",
    "    \n",
    "    # Download results\n",
    "    if os.path.exists(f\"{drive_save_path}/training_results.png\"):\n",
    "        files.download(f\"{drive_save_path}/training_results.png\")\n",
    "        print(\"âœ… training_results.png downloaded!\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ Check your Downloads folder!\")\n",
    "    print(\"\\nğŸ“‚ Copy to:\")\n",
    "    print(\"   best.onnx â†’ d:/chess-detection-improve/app/model/best.onnx\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(f\"\\nğŸ’¡ Alternative: Download from Google Drive manually\")\n",
    "    print(f\"   Location: {drive_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa60de",
   "metadata": {},
   "source": [
    "## ğŸ’¾ CRITICAL: Save Model ke Google Drive!\n",
    "### âš ï¸ Model di `/content/` akan HILANG saat runtime disconnect!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
